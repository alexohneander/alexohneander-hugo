[{"content":"To briefly explain the situation: We have a HAProxy running on a Debian server as a Docker container. This is the entrance node to a Docker Swarm cluster.\nNow, in the last few days, there have been several small outages of the websites running in the Docker Swarm cluster. After getting an overview, we noticed that no new connections can be established.\nAs soon as we restarted the HAProxy, everything went back to normal. After that I did some research on TCP connections and found out that there is a socket limit.\nIn Linux we have a limit of sockets that can be opened at the same time. At this point, I unfortunately did not understand that this limit refers to a client connection. So we note that a client can establish a maximum of 65535 socket connections to a server.\nThis limit refers to a range of ports that you release. We had about 35k sockets available on our server (HAProxy). Now the pages are always down when this limit is reached. Thinking back for a moment, we should never get to that limit as it relates to a client. But the problem with us was that Docker\u0026rsquo;s softlayer network didn\u0026rsquo;t route the client address cleanly through the NAT, so everything was coming from one client.\nAfter stopping the Docker container and installing HAProxy natively on the server, we were able to cross that boundary as well.\nAssumption Because we NAT all the requests through the Docker network, the source address is always the same. This is how we reach the socket limit. If we omit the NAT and use HAProxy natively, we do not reach this limit, because the source address is no longer always the same.\nConclusion With this setup, we get overhead into the system that we don\u0026rsquo;t need. We have an extra abstraction layer, every request has to go through the Docker network and we reach the socket limit. All these points fall away when we use it natively.\nIf we use a lot of micro services it is important that we use something like Docker, because then we can share the kernel and it makes the deployment much easier.\nBut if we have only one application that is very important, it is better to keep it simple.\n","permalink":"https://alexohneander.de/posts/why-docker-isnt-always-a-good-idea/","summary":"To briefly explain the situation: We have a HAProxy running on a Debian server as a Docker container. This is the entrance node to a Docker Swarm cluster.\nNow, in the last few days, there have been several small outages of the websites running in the Docker Swarm cluster. After getting an overview, we noticed that no new connections can be established.\nAs soon as we restarted the HAProxy, everything went back to normal.","title":"Why Docker isn't always a good idea Part 1"},{"content":"In a freshly set up Kubernetes cluster, we need a so-called CNI. This CNI is not always present after installation.\nWhat is a Container Network Interface (CNI)? CNI is a network framework that allows the dynamic configuration of networking resources through a group of Go-written specifications and libraries. The specification mentioned for the plugin outlines an interface that would configure the network, provisioning the IP addresses, and mantain multi-host connectivity.\nIn the Kubernetes context, the CNI seamlessly integrates with the kubelet to allow automatic network configuration between pods using an underlay or overlay network. An underlay network is defined at the physical level of the networking layer composed of routers and switches. In contrast, the overlay network uses a virtual interface like VxLAN to encapsulate the network traffic.\nOnce the network configuration type is specified, the runtime defines a network for containers to join and calls the CNI plugin to add the interface into the container namespace and allocate the linked subnetwork and routes by making calls to IPAM (IP Address Management) plugin.\nIn addition to Kubernetes networking, CNI also supports Kubernetes-based platforms like OpenShift to provide a unified container communication across the cluster through software-defined networking (SDN) approach.\nWhat is Cilium? Cilium is an open-source, highly scalable Kubernetes CNI solution developed by Linux kernel developers. Cilium secures network connectivity between Kubernetes services by adding high-level application rules utilizing eBPF filtering technology. Cilium is deployed as a daemon cilium-agent on each node of the Kubernetes cluster to manage operations and translates the network definitions to eBPF programs.\nThe communication between pods happens over an overlay network or utilizing a routing protocol. Both IPv4 and IPv6 addresses are supported for cases. Overlay network implementation utilizes VXLAN tunneling for packet encapsulation while native routing happens through unencapsulated BGP protocol.\nCilium can be used with multiple Kubernetes clusters and can provide multi CNI features, a high level of inspection,pod-to-pod connectivity across all clusters.\nIts network and application layer awareness manages packet inspection, and the application protocol packets are using.\nCilium also has support for Kubernetes Network Policies through HTTP request filters. The policy configuration can be written into a YAML or JSON file and offers both ingress and egress enforcements. Admins can accept or reject requests based on the request method or path header while integrating policies with service mesh like Istio.\nPreparation For the installation we need the CLI from Cilium. We can install this with the following commands:\nMac OSx\ncurl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-darwin-amd64.tar.gz{,.sha256sum} shasum -a 256 -c cilium-darwin-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-darwin-amd64.tar.gz /usr/local/bin rm cilium-darwin-amd64.tar.gz{,.sha256sum} Linux\ncurl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz{,.sha256sum} sha256sum --check cilium-linux-amd64.tar.gz.sha256sum sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin rm cilium-linux-amd64.tar.gz{,.sha256sum} Install Cilium You can install Cilium on any Kubernetes cluster. These are the generic instructions on how to install Cilium into any Kubernetes cluster. The installer will attempt to automatically pick the best configuration options for you.\nRequirements Kubernetes must be configured to use CNI Linux kernel \u0026gt;= 4.9.17 Install Install Cilium into the Kubernetes cluster pointed to by your current kubectl context:\ncilium install If the installation fails for some reason, run cilium status to retrieve the overall status of the Cilium deployment and inspect the logs of whatever pods are failing to be deployed.\nValidate the Installation To validate that Cilium has been properly installed, you can run\n$ cilium status --wait /¬Ø¬Ø\\ /¬Ø¬Ø\\__/¬Ø¬Ø\\ Cilium: OK \\__/¬Ø¬Ø\\__/ Operator: OK /¬Ø¬Ø\\__/¬Ø¬Ø\\ Hubble: disabled \\__/¬Ø¬Ø\\__/ ClusterMesh: disabled \\__/ DaemonSet cilium Desired: 2, Ready: 2/2, Available: 2/2 Deployment cilium-operator Desired: 2, Ready: 2/2, Available: 2/2 Containers: cilium-operator Running: 2 cilium Running: 2 Image versions cilium quay.io/cilium/cilium:v1.9.5: 2 cilium-operator quay.io/cilium/operator-generic:v1.9.5: 2 Run the following command to validate that your cluster has proper network connectivity:\n$ cilium connectivity test ‚ÑπÔ∏è Monitor aggregation detected, will skip some flow validation steps ‚ú® [k8s-cluster] Creating namespace for connectivity check... (...) --------------------------------------------------------------------------------------------------------------------- üìã Test Report --------------------------------------------------------------------------------------------------------------------- ‚úÖ 69/69 tests successful (0 warnings) Congratulations! You have a fully functional Kubernetes cluster with Cilium. üéâ\n","permalink":"https://alexohneander.de/posts/baremetal-cni-setup-with-cilium/","summary":"In a freshly set up Kubernetes cluster, we need a so-called CNI. This CNI is not always present after installation.\nWhat is a Container Network Interface (CNI)? CNI is a network framework that allows the dynamic configuration of networking resources through a group of Go-written specifications and libraries. The specification mentioned for the plugin outlines an interface that would configure the network, provisioning the IP addresses, and mantain multi-host connectivity.","title":"Baremetal CNI Setup with Cilium"},{"content":"In this tutorial I will try to explain you briefly and concisely how you can set up a site-to-site VPN for the Google Cloud Network.\nPrerequisites We need 2 virtual machines. The first one on the side of our office and the other one on the side of Google.\nSetup OpenVPN Clients Site-to-Site Client Office Side We need to install OpenVPN, we do it as follows:\napt install openvpn -y After that we add our OpenVPN configuration under this path /etc/openvpn/s2s.conf.\ns2s.conf\n# Use a dynamic tun device. # For Linux 2.2 or non-Linux OSes, # you may want to use an explicit # unit number such as \u0026#34;tun1\u0026#34;. # OpenVPN also supports virtual # ethernet \u0026#34;tap\u0026#34; devices. dev tun # Our OpenVPN peer is the Google gateway. remote IP_GOOGLE_VPN_CLIENT ifconfig 4.1.0.2 4.1.0.1 route 10.156.0.0 255.255.240.0 # Google Cloud VM Network route 10.24.0.0 255.252.0.0 # Google Kubernetes Pod Network push \u0026#34;route 192.168.10.0 255.255.255.0\u0026#34; # Office Network # Our pre-shared static key #secret static.key # Cipher to use cipher AES-256-CBC port 1195 user nobody group nogroup # Uncomment this section for a more reliable detection when a system # loses its connection. For example, dial-ups or laptops that # travel to other locations. ping 15 ping-restart 45 ping-timer-rem persist-tun persist-key # Verbosity level. # 0 -- quiet except for fatal errors. # 1 -- mostly quiet, but display non-fatal network errors. # 3 -- medium output, good for normal operation. # 9 -- verbose, good for troubleshooting verb 3 log /etc/openvpn/s2s.log We also have to enable the IPv4 forward function in the kernel, so we go to /etc/sysctl.conf and comment out the following line:\nnet.ipv4.ip_forward=1 We can then start our OpenVPN client with this command:\nsystemctl start openvpn@s2s On the Office side we have to open the port for the OpenVPN client that the other side can connect.\nSite-to-Site Client Google Side When setting up the OpenVPN client on Google\u0026rsquo;s site, we need to consider the following settings when creating it. When we create the machine, we need to enable this option in the network settings:\nAlso on this side we have to install the OpenVPN client again and then add this config under the path /etc/openvpn/s2s.conf:\n# Use a dynamic tun device. # For Linux 2.2 or non-Linux OSes, # you may want to use an explicit # unit number such as \u0026#34;tun1\u0026#34;. # OpenVPN also supports virtual # ethernet \u0026#34;tap\u0026#34; devices. dev tun # Our OpenVPN peer is the Office gateway. remote IP_OFFICE_VPN_CLIENT ifconfig 4.1.0.2 4.1.0.1 route 192.168.10.0 255.255.255.0 # Office Network push \u0026#34;route 10.156.0.0 255.255.240.0\u0026#34; # Google Cloud VM Network push \u0026#34;route 10.24.0.0 255.252.0.0\u0026#34; # Google Kubernetes Pod Network # Our pre-shared static key #secret static.key # Cipher to use cipher AES-256-CBC port 1195 user nobody group nogroup # Uncomment this section for a more reliable detection when a system # loses its connection. For example, dial-ups or laptops that # travel to other locations. ping 15 ping-restart 45 ping-timer-rem persist-tun persist-key # Verbosity level. # 0 -- quiet except for fatal errors. # 1 -- mostly quiet, but display non-fatal network errors. # 3 -- medium output, good for normal operation. # 9 -- verbose, good for troubleshooting verb 3 log /etc/openvpn/s2s.log We also have to enable the IPv4 forward function in the kernel, so we go to /etc/sysctl.conf and comment out the following line:\nnet.ipv4.ip_forward=1 Connection test Now that both clients are basically configured we can test the connection. Both clients have to be started with systemctl. After that we look at the logs with tail -f /etc/openvpn/s2s-log and wait for this message:\nWed May 5 08:28:01 2021 /sbin/ip route add 10.28.0.0/20 via 4.1.0.1 Wed May 5 08:28:01 2021 TCP/UDP: Preserving recently used remote address: [AF_INET]0.0.0.0:1195 Wed May 5 08:28:01 2021 Socket Buffers: R=[212992-\u0026gt;212992] S=[212992-\u0026gt;212992] Wed May 5 08:28:01 2021 UDP link local (bound): [AF_INET][undef]:1195 Wed May 5 08:28:01 2021 UDP link remote: [AF_INET]0.0.0.0:1195 Wed May 5 08:28:01 2021 GID set to nogroup Wed May 5 08:28:01 2021 UID set to nobody Wed May 5 08:28:11 2021 Peer Connection Initiated with [AF_INET]0.0.0.0:1195 Wed May 5 08:28:12 2021 WARNING: this configuration may cache passwords in memory -- use the auth-nocache option to prevent this Wed May 5 08:28:12 2021 Initialization Sequence Completed If we can\u0026rsquo;t establish a connection, we need to check if the ports are opened on both sides.\nRouting Google Cloud Network After our clients have finished installing and configuring, we need to set the routes on Google. I will not map the Office side, as this is always different. But you have to route the networks for the Google network there as well.\nTo set the route on Google we go to the network settings and then to Routes. Here you have to specify your office network so that the clients in the Google network know what to do.\nIP-Masquerade-Agent IP masquerading is a form of network address translation (NAT) used to perform many-to-one IP address translations, which allows multiple clients to access a destination using a single IP address. A GKE cluster uses IP masquerading so that destinations outside of the cluster only receive packets from node IP addresses instead of Pod IP addresses. This is useful in environments that expect to only receive packets from node IP addresses.\nYou have to edit the ip-masq-agent and this configuration is responsible for letting the pods inside the nodes, reach other parts of the GCP VPC Network, more specifically the VPN. So, it allows pods to communicate with the devices that are accessible through the VPN.\nFirst of all we\u0026rsquo;re gonna be working inside the kube-system namespace, and we\u0026rsquo;re gonna put the configmap that configures our ip-masq-agent, put this in a config file:\nnonMasqueradeCIDRs: - 10.24.0.0/14 # The IPv4 CIDR the cluster is using for Pods (required) - 10.156.0.0/20 # The IPv4 CIDR of the subnetwork the cluster is using for Nodes (optional, works without but I guess its better with it) masqLinkLocal: false resyncInterval: 60s and run kubectl create configmap ip-masq-agent --from-file config --namespace kube-system\nafterwards, configure the ip-masq-agent, put this in a ip-masq-agent.yml file:\napiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: ip-masq-agent namespace: kube-system spec: template: metadata: labels: k8s-app: ip-masq-agent spec: hostNetwork: true containers: - name: ip-masq-agent image: gcr.io/google-containers/ip-masq-agent-amd64:v2.4.1 args: - --masq-chain=IP-MASQ # To non-masquerade reserved IP ranges by default, uncomment the line below. # - --nomasq-all-reserved-ranges securityContext: privileged: true volumeMounts: - name: config mountPath: /etc/config volumes: - name: config configMap: # Note this ConfigMap must be created in the same namespace as the daemon pods - this spec uses kube-system name: ip-masq-agent optional: true items: # The daemon looks for its config in a YAML file at /etc/config/ip-masq-agent - key: config path: ip-masq-agent tolerations: - effect: NoSchedule operator: Exists - effect: NoExecute operator: Exists - key: \u0026#34;CriticalAddonsOnly\u0026#34; operator: \u0026#34;Exists\u0026#34; and run kubectl -n kube-system apply -f ip-masq-agent.yml.\nNow our site-to-site VPN should be set up. You should now test if you can ping the pods and if all other services work as you expect them to.\n","permalink":"https://alexohneander.de/posts/site-to-site-vpn-for-google-kubernetes-engine/","summary":"In this tutorial I will try to explain you briefly and concisely how you can set up a site-to-site VPN for the Google Cloud Network.\nPrerequisites We need 2 virtual machines. The first one on the side of our office and the other one on the side of Google.\nSetup OpenVPN Clients Site-to-Site Client Office Side We need to install OpenVPN, we do it as follows:\napt install openvpn -y After that we add our OpenVPN configuration under this path /etc/openvpn/s2s.","title":"Site to Site VPN for Google Kubernetes Engine"},{"content":"In this post, we will show you how to create a MySQL server backup using Kubernetes CronJobs.\nIn our case, we do not have a managed MySQL server. But we want to backup it to our NAS, so that we have a backup in case of emergency. For this we first build a container that can execute our tasks, because we will certainly need several tasks to backup our cluster.\nCronJob Agent Container First, we\u0026rsquo;ll show you our Dockerfile so you know what we need.\nFROM alpine:3.10 # Update RUN apk --update add --no-cache bash nodejs-current yarn curl busybox-extras vim rsync git mysql-client openssh-client RUN curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.18.0/bin/linux/amd64/kubectl \u0026amp;\u0026amp; chmod +x ./kubectl \u0026amp;\u0026amp; mv ./kubectl /usr/local/bin/kubectl # Scripts RUN mkdir /srv/jobs COPY jobs/* /srv/jobs/ # Backup Folder RUN mkdir /var/backup RUN mkdir /var/backup/mysql Backup Script And now our backup script which the container executes.\nOur script is quite simple, we get all tables with the mysql client, export them as sql file, pack them in a zip file and send them in a 8 hours interval to our NAS.\n#!/bin/bash ############# SET VARIABLES ############# # Env Variables BACKUPSERVER=\u0026#34;8.8.8.8\u0026#34; # Backup Server Ip BACKUPDIR=/var/backup/mysql BACKUPREMOTEDIR=\u0026#34;/mnt/backup/kubernetes/\u0026#34; HOST=\u0026#34;mariadb.default\u0026#34; NOW=\u0026#34;$(date +\u0026#34;%Y-%m-%d\u0026#34;)\u0026#34; STARTTIME=$(date +\u0026#34;%s\u0026#34;) USER=mysqlUser PASS=mysqlPassword ############# BUILD ENVIROMENT ############# # Check if temp Backup Directory is empty mkdir $BACKUPDIR if [ \u0026#34;$(ls -A $BACKUPDIR)\u0026#34; ]; then echo \u0026#34;Take action $BACKUPDIR is not Empty\u0026#34; rm -f $BACKUPDIR/*.gz rm -f $BACKUPDIR/*.mysql else echo \u0026#34;$BACKUPDIR is Empty\u0026#34; fi ############# BACKUP SQL DATABASES ############# for DB in $(mysql -u$USER -p$PASS -h $HOST -e \u0026#39;show databases\u0026#39; -s --skip-column-names); do mysqldump -u$USER -p$PASS -h $HOST --lock-tables=false $DB \u0026gt; \u0026#34;$BACKUPDIR/$DB.sql\u0026#34;; done ############# ZIP BACKUP ############# cd $BACKUPDIR tar -zcvf backup-${NOW}.tar.gz *.sql ############# MOVE BACKUP TO REMOTE ############# rsync -avz $BACKUPDIR/backup-${NOW}.tar.gz root@$BACKUPSERVER:$BACKUPREMOTEDIR # done Kubernetes CronJob Deployment Finally we show you the kubernetes deployment for our agent.\nIn the deployment, our agent is defined as a CronJob that runs every 8 hours. In addition, we have added an SSH key as a Conifg map so that this can write to the NAS and a certain security is given.\napiVersion: batch/v1beta1 kind: CronJob metadata: name: backup-mariadb namespace: default spec: schedule: \u0026#34;0 8 * * *\u0026#34; successfulJobsHistoryLimit: 1 failedJobsHistoryLimit: 1 jobTemplate: spec: template: spec: containers: - name: cronjob-agent image: xxx/cronjob-agent command: [\u0026#34;bash\u0026#34;, \u0026#34;/srv/jobs/backup-mariadb.sh\u0026#34;] volumeMounts: - mountPath: /root/.ssh/id_rsa.pub name: cronjob-default-config subPath: id_rsa.pub - mountPath: /root/.ssh/id_rsa name: cronjob-default-config subPath: id_rsa readOnly: true - mountPath: /root/.ssh/config name: cronjob-default-config subPath: config volumes: - name: cronjob-default-config configMap: name: cronjob-default-config defaultMode: 256 restartPolicy: Never ","permalink":"https://alexohneander.de/posts/backup-mysql-databases-in-kubernetes/","summary":"In this post, we will show you how to create a MySQL server backup using Kubernetes CronJobs.\nIn our case, we do not have a managed MySQL server. But we want to backup it to our NAS, so that we have a backup in case of emergency. For this we first build a container that can execute our tasks, because we will certainly need several tasks to backup our cluster.","title":"Backup MySQL Databases in Kubernetes"},{"content":"DevOps Engineer, deineApotheke seit 2020\n‚Äûdeine Apotheke‚Äú unterst√ºtzt die Apotheken in deiner Nachbarschaft und ebnet dir den Weg zu den Apotheken-Services: √úber unsere App kannst du deine Apotheke ausw√§hlen und dort Medikamente, auch mit Rezept, vorbestellen.\nBereitstellung diverser Services (u.A. Backend APIs, MariaDB Cluster, NATs, Redis) Konzeption und Betrieb Kubernetes Cluster (3 Standorte) Verwaltung automatisierter Pipelines via Bitbucket Pipelines (Continous integration) IT-Administration f√ºr 6 Personen (SysOps) Produkt Webseite\nDevOps Engineer, Amamed seit 2020\nF√ºr Ihre Apotheke genau das Richtige! amamed ist die einzige digitale L√∂sung am Markt, bei der Ihre Apotheke im Mittelpunkt steht und mit der Sie online vollumf√§nglich, sicher und flexibel aufgestellt sind.\nBereitstellung diverser Services (u.A. Reverse Proxys, Datenbanken, Load Balancers) Betrieb Docker Swarm Cluster Produkt Webseite\nFullstack .Net Developer, prointernet bis 2019\nAgentur f√ºr Internet und Design 1998 gegr√ºndet, etabliert in Kastellaun im Hunsr√ºck, weltweit t√§tig und im Internet zu Hause. Ein Team aus Designern, Entwicklern und Beratern, die lieben was sie tun.\nEntwicklung von Web-Applikationen (C#, Dotnet, JS) Gestaltung von Webseiten (Composite C1) Firma Webseite\n","permalink":"https://alexohneander.de/experience/","summary":"DevOps Engineer, deineApotheke seit 2020\n‚Äûdeine Apotheke‚Äú unterst√ºtzt die Apotheken in deiner Nachbarschaft und ebnet dir den Weg zu den Apotheken-Services: √úber unsere App kannst du deine Apotheke ausw√§hlen und dort Medikamente, auch mit Rezept, vorbestellen.\nBereitstellung diverser Services (u.A. Backend APIs, MariaDB Cluster, NATs, Redis) Konzeption und Betrieb Kubernetes Cluster (3 Standorte) Verwaltung automatisierter Pipelines via Bitbucket Pipelines (Continous integration) IT-Administration f√ºr 6 Personen (SysOps) Produkt Webseite\nDevOps Engineer, Amamed seit 2020","title":"Erfahrung"},{"content":"","permalink":"https://alexohneander.de/imprint/","summary":"","title":"Impressum"}]